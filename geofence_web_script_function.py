# -*- coding: utf-8 -*-
"""Copy of Geofence Web Script Function.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O7xMjXPPcAOPS7r53q_L6kO1Bkf4Z1jx
"""

# mlp for binary classification
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import Normalizer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.utils import plot_model
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import TensorBoard
# from matplotlib import pyplot
# import datetime, os
import pathlib
import json
import requests
import pandas as pd
from pandas import read_csv
import sys
import os

"""Creating New Geofences Models"""
def generate_geomodel(csvfile_path, model_name, api_link):
  serve_path = 'https://wms.cardi-hu.com/files/geofenceModels/'
  preprocess_name_org = model_name + ".txt"
  model_name_h5_org = model_name + ".h5"
  model_name_tf_org = model_name + ".tflite"
  df_p = read_csv(csvfile_path, header=0)

  # the directory were the script is (ie /var/www/html)
  here = os.path.abspath(os.path.dirname(__file__))
  # now we can build the path
  preprocess_name = os.path.join(here, preprocess_name_org)
  model_name_h5 = os.path.join(here, model_name_h5_org)
  model_name_tf = os.path.join(here, model_name_tf_org)
  
  df_p.fillna("-100",inplace=True)
  colname = df_p.columns[0]
  if colname == 'WIFI_0_BSSID':
    feature_bssid = ["WIFI_0_BSSID", "WIFI_1_BSSID", "WIFI_2_BSSID", "WIFI_3_BSSID", "WIFI_4_BSSID"]

    try:
      df_p.drop("MAGNETIC_FIELD_uT", axis = 1 ,inplace = True)

    except:
      df_p.drop("MF", axis = 1 ,inplace = True)
      
  else:
    feature_bssid = ['bssid_0','bssid_1','bssid_2','bssid_3','bssid_4']
  
  encoders = {}
  mapping_enconding = []
  for feature in feature_bssid:     
    encoders[feature] = preprocessing.LabelEncoder()
    df_p[feature] = encoders[feature].fit_transform(df_p[feature])
    mapping = dict(zip(encoders[feature].classes_, range(0, len(encoders[feature].classes_))))
    #print(encoders[feature].classes_)
    mapping_enconding.append(mapping)

  X, y = df_p.values[:, :-1], df_p.values[:, -1]

  X = X.astype('float32')

  le_y = preprocessing.LabelEncoder()
  y = le_y.fit_transform(y)

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=5)

  sc = StandardScaler()
  X_train[:, :] = sc.fit_transform(X_train[:, :])
  X_test[:, :] = sc.transform(X_test[:, :])
  n_features = X_train.shape[1]


  mean = list(sc.mean_)
  sd = list(sc.scale_)

  # Data to be written
  dictionary ={
    "data": mapping_enconding,
    "mean" : mean,
    "standardDeviation": sd 
  }
    
  # Serializing json 
  json_object = json.dumps(dictionary, indent = 4)
    
  # Writing to sample.json
  with open(preprocess_name, "w") as outfile:
    outfile.write(json_object)

  # define model
  model = Sequential()
  model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))
  #model.add(Dropout(0.4))
  model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))
  #model.add(Dropout(0.4))
  model.add(Dense(1, activation='sigmoid'))
  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
  mc = ModelCheckpoint(model_name_h5, monitor='val_loss', mode='min', verbose=1, save_best_only=True)
  history = model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=1,validation_split=0.1,callbacks=[es,mc])
  model.save(model_name_h5)
  #test_url = "https://wms.cardi-hu.com/api/auth/admin/geofences/upload-files"


  # Convert the model.
  export_dir = 'saved_model/1'
  tf.saved_model.save(model, export_dir)
  converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
  tflite_model = converter.convert()
  tflite_model_file = pathlib.Path(model_name_tf)
  tflite_model_file.write_bytes(tflite_model)

  test_files = {
    "model_file": open(preprocess_name, "rb"),
    "signature_file": open(model_name_tf, "rb")
  }

  test_response = requests.post(api_link, files = test_files)

  if test_response.ok:
    print("Upload completed successfully!")
    print(test_response.text)
  else:
    print("Something went wrong!")




generate_geomodel(sys.argv[1], sys.argv[2],sys.argv[3])
# generate_geomodel ()
